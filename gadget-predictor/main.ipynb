{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all libs.\n",
        "import os\n",
        "import requests\n",
        "from duckduckgo_search import DDGS\n",
        "from pathlib import Path\n",
        "import time\n",
        "import dask\n",
        "\n",
        "# Constants.\n",
        "DATASET_DIR = Path(os.getcwd()) / \"datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = None\n",
        "# Wanb init.\n",
        "def init_wandb():\n",
        "    try:\n",
        "        import wandb\n",
        "        global run\n",
        "        # Start a new wandb run to track this script.\n",
        "        run = wandb.init(\n",
        "            # Set the wandb entity where your project will be logged (generally your team name).\n",
        "            entity=\"prinzz-personal\",\n",
        "            # Set the wandb project where this run will be logged.\n",
        "            project=\"gadgets-predictor\",\n",
        "            # Track hyperparameters and run metadata.\n",
        "            config={\n",
        "                \"learning_rate\": 0.02,\n",
        "                \"architecture\": \"CNN\",\n",
        "                \"dataset\": \"images\",\n",
        "                \"epochs\": 10,\n",
        "            },\n",
        "        )\n",
        "    except ImportError:\n",
        "        print(\"wandb is not installed. Skipping wandb initialization.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during wandb initialization: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_images(query, output_dir, max_results=50):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Check if output_dir already has enough images\n",
        "    existing_files = [\n",
        "        f for f in os.listdir(output_dir)\n",
        "        if os.path.isfile(os.path.join(output_dir, f))\n",
        "    ]\n",
        "    if len(existing_files) >= max_results:\n",
        "        print(f\"Skipping '{query}': {len(existing_files)} images already present.\")\n",
        "        return\n",
        "\n",
        "    ddg = DDGS()\n",
        "    results = ddg.images(query, max_results=max_results)\n",
        "\n",
        "    downloaded = len(existing_files)\n",
        "    for idx, result in enumerate(results):\n",
        "        if downloaded >= max_results:\n",
        "            break\n",
        "        image_url = result[\"image\"]\n",
        "        try:\n",
        "            response = requests.get(image_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            ext = image_url.split(\".\")[-1].split(\"?\")[0][:4]\n",
        "            filename = os.path.join(output_dir, f\"{query.replace(' ', '_')}_{downloaded}.{ext}\")\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "            downloaded += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {image_url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dask.distributed import Client\n",
        "client = Client(threads_per_worker=os.cpu_count() // 2, n_workers=os.cpu_count())\n",
        "gadgets = [\"smartphone\", \"tablet\", \"smartwatch\", \"headphones\", \"camera\"]\n",
        "parallel_results = []\n",
        "for gadget in gadgets:\n",
        "    parallel_result= dask.delayed(download_images)(gadget, output_dir=DATASET_DIR / gadget, max_results=200)\n",
        "    parallel_results.append(parallel_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parallel_results = dask.compute(*parallel_results)\n",
        "print(\"All downloads completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def validate_images(directory):\n",
        "  invalid_images = []\n",
        "  for root, _, files in os.walk(directory):\n",
        "    for file in files:\n",
        "      file_path = os.path.join(root, file)\n",
        "      try:\n",
        "        with Image.open(file_path) as img:\n",
        "          img.verify()  # Verify if the file is a valid image\n",
        "      except Exception as e:\n",
        "        invalid_images.append(file_path)\n",
        "        print(f\"Invalid image: {file_path} - {e}\")\n",
        "  return invalid_images\n",
        "\n",
        "invalid_files = validate_images(DATASET_DIR)\n",
        "print(f\"Number of invalid images: {len(invalid_files)}\")\n",
        "\n",
        "# Remove invalid images\n",
        "for invalid_file in invalid_files:\n",
        "    try:\n",
        "        os.remove(invalid_file)\n",
        "        print(f\"Removed invalid image: {invalid_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to remove {invalid_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def convert_to_rgb(img):\n",
        "    if img.mode in (\"P\", \"RGBA\", \"LA\"):\n",
        "        return img.convert(\"RGB\")\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(convert_to_rgb),  # Convert palette/transparency images\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Load dataset\n",
        "full_dataset = datasets.ImageFolder(DATASET_DIR, transform=transform)\n",
        "\n",
        "# Set split ratios\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)        # 70% training\n",
        "val_size = int(0.15 * total_size)         # 15% validation\n",
        "test_size = total_size - train_size - val_size  # 15% test\n",
        "\n",
        "# Split the dataset randomly\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Use pre-trained ResNet\n",
        "# For torchvision >= 0.13, use 'weights' instead of 'pretrained'\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(full_dataset.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Train\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping parameters\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_wandb()\n",
        "for epoch in range(20):  # Set a high max epoch, early stopping will halt if needed\n",
        "    global run\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"[Train] Epoch {epoch+1} - Loss: {running_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"[Val]   Epoch {epoch+1} - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "    run.log({\"train_acc\": train_acc, \"train_loss\": running_loss,\"val_acc\": val_acc, \"val_loss\": val_loss})\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Optionally save the best model here\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get class names from the dataset\n",
        "class_names = test_loader.dataset.dataset.classes  # Note: .dataset twice due to Subset\n",
        "\n",
        "# Get one batch\n",
        "inputs, labels = next(iter(test_loader))\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "outputs = model(inputs)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "\n",
        "# Move to CPU for visualization\n",
        "inputs = inputs.cpu()\n",
        "preds = preds.cpu()\n",
        "\n",
        "# Show images with predicted labels\n",
        "def imshow(img_tensor):\n",
        "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(min(8, len(inputs))):\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    imshow(inputs[i])\n",
        "    plt.title(f\"Pred: {class_names[preds[i]]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exporting and Using the Trained Model for Inference\n",
        "This section shows how to save your trained model and use it for inference on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model for inference\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model architecture (must match training)\n",
        "NUM_CLASSES = len(full_dataset.classes)  # Or set manually if needed\n",
        "model = models.resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "print('Model loaded and ready for inference.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference on a new image\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Use the same preprocessing as validation/test\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Lambda(convert_to_rgb),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load and preprocess the image\n",
        "img_path = DATASET_DIR/ 'smartphone' / 'smartphone_0.jpg'  # Change to your image path\n",
        "\n",
        "img = Image.open(img_path)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img_tensor = inference_transform(img).unsqueeze(0)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(img_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Map prediction to class name\n",
        "class_names = full_dataset.classes  # Or load your class names list\n",
        "print(f'Predicted class: {class_names[predicted.item()]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
